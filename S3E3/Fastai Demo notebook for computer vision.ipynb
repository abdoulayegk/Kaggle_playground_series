{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# **Computer Vision using FastAI**\n",
    "\n",
    "This notebook will use the FastAI library to complete a computer vision task. \n",
    "\n",
    "I cannot reccommend the FastAI course highly enough. It's a great entry point in to Neural Networks and Deep Learning in general. Many of the explanations I provide below are snippets from the course.\n",
    "\n",
    "You can find more information here:\n",
    "\n",
    "https://www.fast.ai/\n",
    "\n",
    "and the course here:\n",
    "\n",
    "https://course.fast.ai/\n",
    "\n",
    "I reccommend using Google Collab for the course, rather the a jupyter notebook.\n",
    "\n",
    "# **Project: Pneumonia Prediction**\n",
    "\n",
    "The problem: **Can we correctly classify chest X-Rays of patients that have pneumonia & those that do not?**\n",
    "\n",
    "# Project plan\n",
    "\n",
    "Here are the steps I plan to follow in this project:\n",
    "\n",
    "* Import libraries & data\n",
    "\n",
    "* Data overview - Show a few images, check classes, length of dataset etc.\n",
    "\n",
    "* Train a model as a baseline\n",
    "\n",
    "* Assess model misclassifications\n",
    "\n",
    "* Model tuning:\n",
    "    - Plot learning rates\n",
    "    - Unfreeze the ResNet model\n",
    "    - Use discriminative learning rates\n",
    "    \n",
    "It's also worth noting that I am following the above steps in order to gain more experience with FastAI - the particular steps I take may or may not be neccesary, I just want to utilise the various tools that FastAI offers.\n",
    "\n",
    "\n",
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "import numpy as np\n",
    "import torch\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "\n",
    "I'll get the path for the dataset. This saves reading in data in the usual way, clogging up your notebook with thosands of image paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "path = Path('/kaggle/input/chest-xray-pneumonia/chest_xray/'); path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader is a class that provides batches of a few items at a time - let's do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock), \n",
    "    get_items=get_image_files, \n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    get_y=parent_label,\n",
    "    item_tfms=Resize(128))\n",
    "\n",
    "dls = data.dataloaders(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch(max_n=12, nrows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from these images will be noted below. First, I'll do some more checks to confirm our **categories** are just 'Normal' and 'Pneumonia':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our **dataset length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dls.train_ds), len(dls.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "To the **untrained** eye it is difficult to say with certainty, looking at an X-ray, what visual distinctions there are between 'Normal' and those with 'Pneumonia'.\n",
    "\n",
    "Perhaps the larger white mass in the chest is indicative of pneumonia, but this is conjecture on my part.\n",
    "\n",
    "Also, our validation set is quite a lot smaller than our training set, but this isn't a problem. Actually, we often want our training set to be as large as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "Let's train a model.\n",
    "\n",
    "FastAI has several ResNet models in readily available:\n",
    "\n",
    "* Resnet18\n",
    "* Resnet34\n",
    "* Resnet50\n",
    "* Resnet101\n",
    "* Resnet152\n",
    "\n",
    "They vary in size, with ResNet152 being trained on the most amount of images. \n",
    "\n",
    "For the basline, I will use **ResNet18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=[error_rate,accuracy])\n",
    "learn.fine_tune(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll save this model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fantastic results.**\n",
    "\n",
    "This is often why **training a model quickly is a good place to start**. \n",
    "\n",
    "Why? Well, it prompts a number of questions:\n",
    "\n",
    "* Is this score good enough already? \n",
    "\n",
    "* Do we have time to train more models? \n",
    "\n",
    "* Is optimization necessary?\n",
    "\n",
    "These are all questions the may or may not provide direction in your specific domain. \n",
    "\n",
    "A fantastic course relevant to my point above is **\"Structuting Machine Learning Projects\"** by **Andrew Ng** on Coursera. \n",
    "\n",
    "It focuses on the practical realities of ML tools & products, optimization, setting a basline, and it is a fantastic resource. \n",
    "\n",
    "Here's a link:\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning-projects\n",
    "\n",
    "\n",
    "\n",
    "In a medical domain, I'd say it is worth spending time to see if we can optimize further, but in some cases, a model such as the one above would be more than sufficient.\n",
    "\n",
    "Now, where did we go wrong (and right)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see here that the **model did very well**. \n",
    "\n",
    "The main issuse the model has is producing **False Positives**. 20 Patients would be predicted to have pneumonia when in fact they do not. However, the more important metric in this case is **False Negatives**, of which there are 3. Not perfect, but a very good result overall.\n",
    "\n",
    "We do note that we got some incorrect, we can view these explicitly.\n",
    "\n",
    "Here, I'll choose the **top 3 losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(3, nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be an opportunity to *fix* issues in your dataset. For instance, if an image is incorrectly labelled. \n",
    "\n",
    "If you're using scraped data, for instance, this can be a problem. I built a classifier that can classify the Big Cats of Africa, and the amount of domestic cats that snuck in to the dataset was a real headache, so this can be a very important & useful step.\n",
    "\n",
    "This relies on **domain expertise** though, so for this case I wouldn't attempt it. However, it is curious that the large white mass in the chest area on those incorrectly classified above do look very similar to the test batch we showed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the model\n",
    "\n",
    "Adjusting the learning rate. If our learning rate is too low, it can take an age train our model. In addition, it means that we might overfit, because every time we do a complete pass through the data, we give our model a chance to memorize it. \n",
    "\n",
    "Seeing as the model did so well with ResNet18, I will continue to use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=error_rate)\n",
    "lr_min,lr_steep = learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the Learner\n",
    "\n",
    "The aim is to pick a learning rate that is not too high, and not too low.\n",
    "\n",
    "The graph above helps us to ensure we acheive that.\n",
    "\n",
    "Ideally, we want to pick a point where the Loss is still decreasing, and has not started to increase. Above, that happens at around 10^-1.\n",
    "\n",
    "The decrease in Loss starts around 10^-4. I will pick between 10^-3 and 10^-2. \n",
    "\n",
    "Note that the Learner is on a logarithmic scale, which is why the middle point between 10^-3 and 10^-2 is between 3e-3 and 4e-3. \n",
    "\n",
    "Let's **re-train the model** for the same number of epochs but with the **new learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=[error_rate,accuracy])\n",
    "learn.fine_tune(6, base_lr=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this model, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, our model has **improved our accuracy due to changing the learning rate**.\n",
    "\n",
    "As an aside, the default value that FastAI uses is 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also normalize this plot to view the percentages.\n",
    "\n",
    "This can be useful to gain perspective. \n",
    "\n",
    "For instance, we now see that our **recall is close to 100%** and that **False Negatives only occur 0.02% of the time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(normalize=True, norm_dec=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very good. We managed to **reduce the number of False Positives**. \n",
    "\n",
    "However the False Negatives are still the same. \n",
    "\n",
    "**We did improve the model** though, which was our aim.\n",
    "\n",
    "These misclassifications are shown below explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused(min_val=1)\n",
    "\n",
    "# Shown is 'ACTUAL, PREDICTED, COUNT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's view the **top 3 losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(3, nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many epochs to train for?\n",
    "\n",
    "Your first approach to training should be to simply pick a number of epochs that will **train in the amount of time you're willing to wait**.\n",
    "\n",
    "Then look at the training and validation loss plots and in particular **your metrics**. If you see that they are still getting better even in your final epochs, then you know that you have not trained for too long.\n",
    "\n",
    "**Remember, it's not just that we're looking for the validation loss to get worse, but the actual metrics**. This is an important realisation and can change how you approach machine learning & deep learning problems. If your desired metrics are improving, on the validation or test set, then that is the most important factor.\n",
    "\n",
    "The loss function is just something that we use to allow our optimizer to have something it can differentiate and optimize; it's not actually the thing we care about in practice.\n",
    "\n",
    "# Unfreezing the model\n",
    "\n",
    "A pre trained model, such as the ResNet model we are using here, can be used on data other than the data it was trained on - this is **transfer learning**\n",
    "\n",
    "\n",
    "\n",
    "Because the model was trained on some other dataset, we might be able to improve it by effectivley removing the final linear layer of the model - which is specifically designed to classify the categories in the original pretraining dataset - and replace it with a layer specific to our dataset.\n",
    "\n",
    "\n",
    "From FastAI:\n",
    "\n",
    "*We want to train a model in such a way that we allow it to remember all of these generally useful ideas from the pretrained model, use them to solve our particular task, and only adjust them as required for the specifics of our particular task.*\n",
    "\n",
    "*Our challenge when fine-tuning is to replace the random weights in our added linear layers with weights that correctly achieve our desired task without breaking the carefully pretrained weights and the other layers. There is actually a very simple trick to allow this to happen: tell the optimizer to only update the weights in those randomly added final layers. Don't change the weights in the rest of the neural network at all. This is called freezing those pretrained layers.*\n",
    "\n",
    "When we call the fine_tune method FastAI does two things:\n",
    "\n",
    "* Trains the randomly added layers for one epoch, with all other layers frozen\n",
    "* Unfreezes all of the layers, and trains them all for the number of epochs requested\n",
    "\n",
    "Let's try that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=[error_rate,accuracy])\n",
    "learn.fit_one_cycle(3, 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, unfreeze the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find a new learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, lr_max=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model didn't perform as well as the models above. \n",
    "\n",
    "I will now try another method..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Learning Rates\n",
    "\n",
    "FastAI lets you pass a Python slice object anywhere that a learning rate is expected. The first value passed will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. \n",
    "\n",
    "I'll select a range here based on the plot above. \n",
    "\n",
    "I will then train for a while to see how the model performs. When our scores are already so high, it is hard to improve, so I am not expecting much improvement, if any, but I want to try this additional step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=[error_rate,accuracy])\n",
    "learn.fit_one_cycle(3, 5e-3)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, lr_max=slice(5e-3,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We certainly **didn't improve on our earlier results** - although 98% accuracy still can't be considered a poor result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "We were able to acheive an **accuracy score of over 99%**\n",
    "\n",
    "This was accompanied by high precision & recall, with close to 100% of those with Pneumonia being detected successfully.\n",
    "\n",
    "First, I set a baseline - which scored incredibly highly at the outset. From there, I knew it would be a challenge to improve the score.\n",
    "\n",
    "Next, I adjuted the learning rate of the model and managed to improve the accuracy by nearly a full percentage point. This happens to be our final model.\n",
    "\n",
    "I also tried unfreezing the base model, and also implementing disriminative learning rates. Though both of these still scored highly, neither matched up to earlier results. \n",
    "\n",
    "Overall, I am happy the the model is a success.\n",
    "\n",
    "# **Thank you for reading**\n",
    "\n",
    "Please consider upvoting if you found this notebook helpful.\n",
    "\n",
    "# Some of my other work\n",
    "\n",
    "**EDA & Prediction using SMOTE [HR Dataset]**\n",
    "\n",
    "https://www.kaggle.com/joshuaswords/awesome-hr-data-visualization-prediction\n",
    "\n",
    "\n",
    "**Stroke Prediction & Model Interpretation with SHAP, LIME, and ELI5**\n",
    "\n",
    "https://www.kaggle.com/joshuaswords/predicting-a-stroke-shap-lime-explainer-eli5\n",
    "\n",
    "\n",
    "**Beautiful EDA & Clustering [World Happiness Index 2021]**\n",
    "\n",
    "https://www.kaggle.com/joshuaswords/awesome-eda-2021-happiness-population\n",
    "\n",
    "\n",
    "**EDA & Gold Price Prediction using Prophet**\n",
    "\n",
    "https://www.kaggle.com/joshuaswords/eda-gold-price-prediction-prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
